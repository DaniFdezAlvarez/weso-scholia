# Meeting 15-June-2021

## Attendees 

- Labra
- Dani
- Pablo
- Guillermo
- Jorge

## Discussions
- Problems with size of Wikidata dumps
- Should we maintain historic dumps? If so, how many? 
- Subset of wikidata and blazegraph 
    - Perfomance measurements
    - Country-authors
    - Federated queries give different results
    - Undeterministic results (apparent 6000 rows limit in some iterations)
- Research on named solution sets (INCLUDE) performance
    - Hypothesis...the subset returned is too big?
    - Could it work if we return partial results?
    - Example: researchers per country (country_authors) creates a subset with every person associated to each nation and checks the existence of publications per all of them, which in the case of large countries produces a timeout.
    - If we go with partial results, is there a way to select important nodes instead of selecting a random slice of the original set of nodes? PageRank (Andreas Thalhamer) or similar metrics precomputed in Wikidata? https://stackoverflow.com/questions/39438022/wikidata-results-sorted-by-something-similar-to-a-pagerank/46797845

## Tasks
- Guillermo: prepare hardware specs and budget to handle wikidata copies
    - 1/2 active copies of wikidata 
    - . . .
    - Budget for HDDs for our WESO server
    - Budget for a whole server (check what spec requirements we need)
- Pablo continue working with federated queries
    - Continue working on subsetting
    - Check which entities are obtained/lost
    - Check how to merge entities in Fuseki
- Jorge: 
    - Refactor Country_authors to avoid excesivily large subsets (LIMIT, OFFSET)
    - Refactor to add to our dockerized Scholia  
- 
